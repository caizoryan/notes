thank you for joining us for this special talk event between and Trevor pagam who are two of the artists
0:12
featured in universal remote my name is y here and I am a
0:18
special researcher at the national Arts Center and I devised and created this
0:25
exhibition
0:30
today's discussion is a continuation of the talk
0:35
that the two had at the pompy door in 2018 which is included in this cat in
0:45
the catalog of this exhibition I just like to summarize that in
0:51
2018 the two had a discussion about images AI surveillance technology facial
1:01
recognition are a few examples of what AI had achieved by that point also AI
1:07
technology and how images are impacted from very close toand examples such as
1:13
smartphones as part of the discussion and I just like to summarize
1:19
some of the key points from that discussion in 2018 TR pagin was talking
1:25
about the emergence of the invisible images images by definition are visible to the eye
1:33
however AI has Now hidden images from Human View and human
1:40
sight and with surveillance cameras and facial recognition as a few examples going
1:47
beyond that as we live in contemporary Society we are always being seen by
1:52
various machines and the universal remote
1:58
features a work features work by Trevor which is utilizing Ai and this was uh
2:06
created in 2017 year before the Pomo talk and looking at that you can see the
2:14
world which is seen by AI through the learning process where the AI teach each other
2:22
AI suction of and draw up vast quantities of information from human
2:28
beings and they consume vast quantities of images and also information but um this which goes far
2:35
beyond anything achievable by any single individual is also politicized and
2:40
becomes political the basis of a technology is physical infrastructure and there of
2:46
course you also have a political thought and also objectives the emergence of the
2:53
invisible images made possible by AI the implications for Global power techn uh
2:59
Power Global political power and also global capitalism is what was touched
3:04
upon by Trevor with d it was about artificial intelligence or rather
3:11
artificial stupidity and how it surrounds us all the danger of becoming
3:17
too accustomed to that is what was talking about for example looking at
3:27
smarts we are could to be enslaved by
3:33
standards of beauty which are at the same time discriminatory for example whitening or also clearing your
3:40
complexion and not just smartphones is also is part of what surrounds us an everyday basis which is almost magical
3:48
in terms of how much it can permeate and also penetrate our awareness because of
3:55
that it is fundamentally destructive also AI is not just promoting the
4:01
consumption of images it also consumes energy and also electrical power is also
4:07
part of what is talking about also when you look at nation
4:12
states and capitalism and what they also have in mind and with the machine
4:19
learning of AI and also the possibility that things may language may
4:26
become privatized or monopolized and also be appropriated or stolen and so image and power are
4:35
inseparable it also promotes us and also drives us and is concerned about the
4:41
development and also maturation of Technology including Ai and the global
4:47
tendency and so with the far right and also
4:52
authoritarian Powers which may possibly been part by this is part of what the two were concerned about
5:00
and at the very end of that discussion they said that they need to talk about this further in 24 I think that the
5:07
discussion is still important and also relevant AI is now able to carry out
5:14
conversations with human beings which sound natural emotionally speaking they
5:23
also uh have implications for de deciding or determining rather the
5:29
future of Elections and the outcome of Wars now that there are fake um news Clips or video footage and also the
5:37
scops which are going on and information Wars and so the situation has actually
5:43
become even more Fierce and even more frightening technology is astonishingly
5:48
developed but human beings have not changed and they have not learned is what comes to
5:55
mind the autonomy of images or we only these images can kill but now they fingers are on the triggers was the
6:01
title of the debate that they had or discussion they had in pompo but now the
6:09
trigger the and now the trigger has not been um actually pressed upon but six is
6:15
now later perhaps the trigger has been pressed down and so after the pandemic
6:20
the situation is even more conf confusing and we think it's going to be a very interesting discussion that we're
6:26
going to be having and so I'd
6:35
now like to have speak about her recent work for about 15
6:46
minutes and after that Trevor will be speaking for about
6:52
15 minutes and then there'll be a 45 minute
6:58
discussion and a five minute q& day and we'll be ending at
7:13
3:30 I should also introduce these speakers
7:19
was born in Munich Germany in 1966 lives and works in
7:24
Berlin she studied documentary film at the Japan Japan insute of the movie
7:29
image and the University of television and film Munich is the holder of a PhD
7:34
in Philosophy from The Academy of Finance Vienna in 2003 key s exhibitions
7:42
include hyle the city of broken windows at
7:47
liik and the Sea of data at the national museum
7:53
of modern and Contemporary Art career Soul she's also participated in the art Meo group
8:00
exhibition hello world for the posthuman age and also I should mention that dutyfree
8:07
art has been translated and published in Japanese in
8:16
21 so the microphone is yours thank you so much J here for the
8:21
wonderful show and for the generous introduction and also for the invitation here I very much appreciate it thank you
8:29
also to Trevor especially to borrowing your glasses to me so today we will
8:36
exactly look the same and I will see the world with Trevor's eyes I think um yeah
8:42
I have to I cannot put it up otherwise I cannot um so what I wanted to do is can
8:49
we maybe see no okay that's the wrong one that this is so what I'm going to do
8:54
is to start talking a little about the work you can see downstairs
9:00
uh Bal mission accomplished balance um just to give you some context
9:05
about it and then I will do a brief maybe introduction of where I think we
9:11
are at right now in relation to digital images just as a brief introduction to
9:19
uh Travis and my discussion later so this is basically a recording of another
9:26
variation of the installation you see downstairs which is called mission accomplished
9:32
Balon um was made in 2019 right before the pandemic it was
9:38
the last work we could still make before the pandemic and it's a collaborative
9:44
work made in collaboration with two young artists called Gago gagos and
9:49
Milos trovich Milos is from Bosnia originally and Gago is from
9:56
Georgia and it's a so it's a lecture video lecture let's call it that deals
10:03
with the topic of fashion uh in relation
10:09
to world history and digital technology
10:14
so um we took basically the example of fashion brand
10:19
Balenciaga as one example to discuss the period after the coming down of the
10:26
Berlin Wall in 1989 and today to see how basically not only
10:33
fashion but also digital technology but also politics have changed until then
10:39
and Balenciaga is a very interesting example where you can see all of these
10:45
topic somehow coming together um why is this because
10:51
Balenciaga actually is produces some kind of data
10:56
fashion right it's a fashion which is heavily based on extracting data on
11:03
Instagram from its users its fans its followers other people who are
11:09
interested in fashion and sort of mashing them up into some kind of meme fashion that's the one thing but um we
11:19
were also trying to compare this with the overall political developments in
11:24
Europe after the coming down of the Berlin War and the collapse of the
11:30
Soviet Union um because in the region of the Eastern Europe a similar kind of
11:37
extraction was taking place in the in the guise of so-called
11:43
privatization whatever was owned by the state before or by you know companies
11:50
before was sort of transferred to private investors in many cases Western
11:57
investors who were BAS basically extracting this kind of
12:04
resources um from the area so we are trying to link this era of
12:11
privatization in Eastern Europe to the fashion making um procedure of
12:20
Balenciaga and what happened in Eastern Europe was that this type of privatization created a new political
12:28
Elite very often also criminal Elite uh but also oligarch Elite which very much
12:37
likes fashion you know not only Balenciaga but also all other kinds of
12:43
luxury brands are very much appreciated by this new Elite and uh specifically
12:50
Balenciaga has taken a lot of inspiration from famous historical
12:56
photos from that period there is for example one one example from the period
13:02
of The saru Siege um where a woman
13:08
during the Yugoslav War Saro was being besieged by Serbian forces and there was
13:15
one famous news photograph in which a woman walked down a
13:20
street um which was um um targeted by snipers this is is the
13:29
photo very um famous news photo from that period and she seemed completely
13:35
unconcerned and was wearing a dress as if she was on a catwalk and this um specific dress
13:43
reappeared later in a Balenciaga collection um on on the on the catwalk
13:51
so basically this kind of inspiration from world history and from
13:56
Fashion sources here you see the basically copypaste version that in which a
14:03
historical situation which was experienced under extreme duress is sort
14:10
of well it's commodified it's turned into some sort of style into a slightly
14:16
edgy style etc etc the same thing happened of course with um workers
14:22
uniforms DHL uniform is a very um wellknown example Ika BS Bernie Sanders
14:29
inspired fashion and so on and so on so basically one of the propositions which
14:34
especially my colleague um Gago gagos who's also from Georgia the main
14:40
designer of Balenciaga is from Georgia as well describes as the commodification
14:46
of the experience of the transition in which most of the people became actually
14:52
very poor so basically also this Aesthetics of wearing secondhand clothes
14:58
which do not fit properly which are passed down you know from one brother to the sister to the brother etc etc so
15:06
that kind of aesthetic is being appropriated extracted and turned into a
15:12
very um expensive fashion product so
15:17
basically the um work is about describing this type of double
15:23
extraction method on the one hand economical on the ground
15:29
in Eastern Europe extracting resources actual Wells but then also Styles looks
15:37
ways to Lifestyles Etc and then extraction of data from
15:43
social media uh memes Etc to sort of make the
15:49
brand that's basically what the work is about um yeah I don't think a lot has
15:58
changed there even though maybe yes when I just I just
16:04
returned from Shanghai and I had the feeling this luxury braning is not working that well anymore so maybe there
16:12
is a kind of change happening there slowly already but I'm not really sure
16:19
about it yeah so that's about Balenciaga but Balon yeah we called it
16:27
Balon because Milos found a sort of um
16:32
copy product somewhere in Saro in a shopping mall which was very cheap and
16:38
of course copy paste and it was called balance SI so we made balance SI into
16:44
our brand and yeah BOS okay um so maybe this also
16:52
transitions quite nicely into the other part of the talk which is more about
16:58
what writing about now it's more about the state of images at this current
17:04
point in time and uh it also starts basically from a point where all sorts
17:11
of cultural products are made from previous cultural products this is a
17:16
still from a very recent advertisement clip by Apple which had to be retracted
17:24
it was called Crush have you seen it yeah of course you have seen it for to days because then the company had to
17:30
withdraw it because it was so controversial you know that it shows basically all these tools cultural tools
17:38
being flattened by a hydraulic press into what turns out to be a very flat
17:45
and sinin uh iPad and the one one of
17:51
the suggestions is that you know the iPad is going to replace all these
17:57
cultural tools and of course everyone thinks yes it's going to why is it going
18:02
to replace all these cultural tools is this even necessary and it there was a huge let's say dissatisfaction with this
18:10
clip but it raises a bigger question which is what happens when basically all
18:17
the data of all the previous products get flattened and crushed into one flat
18:26
plane and one of the consequences is that every starts looking the same and
18:33
the the Apple clip is actually the best example because it is already copy pasted from a previous um clip which was
18:41
made by LG uh a while ago and it also shows a
18:46
hydraulic press which is flattening all sorts of cultural products so the Apple
18:52
clip is already an example of everything starting to look exactly the same
19:00
um there is a very influential uh text by actually a brand
19:09
strategist called Alex murell who wrote a text which was called the age of
19:15
average and he compared 10,000 brand websites with one another and he found
19:22
out that basically key um key topics all
19:27
look the same so these this is basically cities from these websites and they kind
19:33
of all look the same that that's not even AI That's pre AI or what is this ah
19:42
these are cars it's different brands but still all look the same uh these are toothbrushes
19:52
all in front of a pink background for some reason um cosmetic products and
19:58
this is particularly uh scary this is the so-called Instagram phase in which
20:04
basically people or women in this case all start looking the same because of plastic surgery because of filters etc
20:11
etc they start looking like a sort of human average in a way right you cannot
20:18
exactly tell what kind of skin color it is it is as if you superimposed all the
20:24
faces on the world onto one another and then they start you know finding this
20:29
sort of median point or average point of humanity and of course they look the
20:36
same because this is sort of safe not controversial it completely average no
20:43
one will be of offended uh most probably and as you can probably imagine this um
20:50
effect is much accelerated by AI so these are images generated with m
20:58
Journey on very simple prompts and they show basically exactly the same effect
21:05
this is a white car in a in a wind tunnel looks exactly the same like the
21:11
real image and of course why is this because the AIS are already trained on
21:17
all the average data so they will create more average representations uh this has been also
21:24
called the age of aesthetic consolidation where basically everything
21:29
converges on this sort of normal curve you know you have no more statistical outliers everything converges on this
21:37
kind of mean which is also why I've called this kind of image
21:43
production um mean images because they deal with the average the mean is the
21:51
average of course it also has other meanings and um so there is a lot of
21:58
things that have to be in place for these data collections to come together they don't simply arise out of the blue
22:05
the processes which I've described in relation to Balenciaga are already
22:11
necessary you have to collect a lot of data from social media you have to extract them you have to be able to
22:17
control them to accumulate them you need to have the infrastructure the servers
22:23
the electricity um Etc in order to be to command you know the amount of data
22:31
which you need to produce This Kind of Perfect very boring average which is
22:37
kind of fascinating and the human part or the human labor is kept as invisible
22:44
as possible as in this very famous historical machine it's called The Chess
22:51
Turk the chess robot which was made in the 18th century and it has this little
22:58
person inside um where a human plays chess but everyone thinks that it's a robot which
23:05
knows how to play chess uh that's a very famous example and one of my recent Pro
23:12
projects is to look for this person here and my idea was since this is a chess
23:21
Turk this is probably a chess C and uh so it was I went to C kistan
23:29
Kurdistan is a region which stretches across four different countries but this
23:35
is a region in north Iraq there's a lot of uh refugee camps
23:40
there since the Syrian War where a lot of people have fled already since
23:46
2011 or they lived there since more than
23:52
10 yeah more than 10 years it's already a very long time uh in these camps and
23:58
the these populations have been discovered as ideal workers for AI
24:04
applications why because people have to prepare the data uh in order for the
24:10
machines to be able to read them they need to annotate the images assign tags
24:15
to them and so on and so on and the World Bank for example discovered people
24:21
in the so-called Mina region as ideal microw workers also in Palestine very
24:27
much t targeted um because yeah of course they were well educated many of
24:35
them went to University but then ended up in this very unfortunate situation
24:41
where they had to accept also very work which was not well paid at
24:46
all
24:54
um yeah the very um paradox iCal thing
24:59
is that these people are mostly doing annotation labor for self-driving cars
25:06
so they tell the car how to orient itself in a street scene they annotate
25:12
scenes from Berlin a lot of you know American cities etc etc so the car can
25:18
learn how to um find its way but of course the people who live there they have no Mobility whatsoever there's not
25:25
even a bus or something like that they cannot cross the border and so on and the most absurd thing is that even the
25:33
cars themselves the self-driving cars like this one apparently they need not
25:38
one driver but 1.5 drivers in the background because
25:44
they get stuck a lot so you think it's a self-driving car which is a robot and
25:50
which doesn't need a driver but in the background there is up to 1.5 workers
25:57
who are helping the car to orient itself so basically all this work of annotating
26:03
the data teaching the car to drive etc etc creates a demand for more people to
26:10
work on these cars which is kind of absurd let me conclude very briefly the
26:17
most interesting aspect for me in generative AI at the moment is um not is
26:24
not even the working conditions is not the images it's not the average Etc it
26:30
is that the most prominent output were protests generative AI has been
26:38
extremely successful in creating protests in different parts of the world
26:43
mainly of artists who oppose AI because they feel threatened by it this is a um
26:51
a photo from the so-called Writers Guild of America strike who were also striking
27:00
against AI being implemented in script writing the actors were also striking
27:07
they were opposing you know um face being face printed so they could be
27:13
replaced by AI generated avatars so basically there has been a lot of labor
27:21
organization not only of artists but also for example of data annotators
27:26
around the world content moderators and so on who are opposed to the social um
27:34
consequences of generative Ai and this is the most prominent um outcome that
27:43
generative AI has created until today a lot of different strikes thank you
27:52
[Applause]
27:58
soito thank you very much we started from this um Valena the
28:06
installation that you have here and then you talked about Instagram faces and micro workers so these are all the uh
28:14
aspects that you're quite interested in the recent days so thank you very much for sharing that with us and then let's
28:19
move on to uh Trevor Pan's presentation let me introduce him uh
28:26
briefly Trevor was born in Maryland United States in
28:32
1974 he currently lives and works in New York and burlin he holds an MFA from the school
28:40
of the Art Institute of Chicago and PhD in geography from the University of California
28:47
Berkeley among his solo exhibitions
28:53
are uh Trevor backlin hide the heat re show the falls at new Berling
28:58
kenin Berlin 2023 and also at uh P Gallery New York and uh he has been
29:06
participating in don't fall the wind a group exhibition that opened in 2015 in the exclusion Zone surrounding the
29:13
Fukushima D nuclear power plant station in Japan and also he was also awarded
29:18
the namjun pike Art Center prize in 2018 for in South Korea and marara fellowship
29:23
USA in 2017 so now Trevor I want to hand it over to you to start with the
29:29
presentation so uh Mr pin please over to
29:34
you so thank you very much um thank you all for coming um thank you for
29:40
organizing this exhibition thank you for including me and thank you for bringing
29:46
um my my good friend hito and us uh together today um I thought I would
29:53
start just by giving a little bit of a background to my work um I'll discuss
29:59
some of the work in the exhibition below and I'll just give a little bit of an
30:04
introduction to how I think about making art um I see my job in a in a very
30:13
simple way and I see my job as being uh as as trying to learn how to
30:20
see the world around us and to try to share that with other people and this is
30:27
a very simple thing for me to say but in practice this can be a very difficult
30:32
thing to do and it can be difficult to do because the world is hard to see um
30:39
many of the things that shape the world around us that shape our societies are
30:45
hidden from us and so I try to learn how to see those
30:51
things I've spent a lot of time trying to see the world of technology and
30:57
technology is such a part of our everyday lives and yet much of what it is and how it works is invisible to
31:06
us and I try to see this world of technology from many different
31:13
perspectives and this effort to try to see this world of technology around us
31:19
has taken me from the bottom of the ocean up into the heavens above our
31:26
heads this is an example of the kind of thing that I mean this is a map that
31:33
Edward Snowden provided to my friend Laura pois while we were working on a film called citizen 4 about Edward
31:42
Snowden and the map describes the physical structure of the internet and
31:47
it describes the methods that the American military uses to take over and
31:53
surveil the internet in different ways you see a lot of different dots on
32:00
this map one of the most important dots are these blue dots and these are the
32:07
places where all of the continents are connected to one another on the internet
32:13
the places where undersea cables connect them so one for one of the projects
32:20
that's shown here I wanted to travel to all of these blue dots I wanted to see
32:25
what these locations look like this is one of
32:31
them and this looks like a normal beach scene but this is one of the most
32:37
abstract images I have ever made in the frame of the photograph there are a huge
32:43
number of undersea cables connecting the east coast of the United States to
32:48
Europe and there's a huge military presence sitting on these cables but
32:54
none of this is visible in the photograph
32:59
this is another abstract photograph of a place in California where the United
33:05
States is connected to Japan and
33:15
Asia in the photographs I have two rules one The Horizon must be in the center of
33:22
the photograph and the second rule is the collection of undersea cables must
33:28
be in the frame of the image and what this implied was that if
33:35
I could dive into the water in the photograph I might be able to see the
33:40
thing that I was actually trying to make an image of and so I started to do
33:49
that I learned how to scoop a dive and learned how to explore the bottom of the
33:56
sea looking for these undersea cables that connect the uh planets continents
34:04
to one another and it turns out that this works
34:09
when you find different spots on the sea you can you can find places where these converg of internet cables do indeed
34:16
connect the continents together this is a place underwater that uh connects
34:22
Japan and Asia to Hawaii um and then continues on to the mainland United
34:27
States States these are other such places and
34:34
this might be funny but this is the internet this is what the internet looks
34:42
like I've spent a lot of time not only looking at the oceans but looking at the
34:49
skies trying to learn how to see the machines that live among the stars above
34:55
us machines that live in the stars above us
35:02
but that are looking down at
35:07
us and I've looked at the clouds photographing unmanned drones
35:13
that are creating new kinds of
35:21
warfare now I said at the beginning of the talk that I see my job as trying to
35:28
learn how to see the world around us and when we set out to see the world
35:35
this happens on at least two axes one axes is just what's out there
35:44
but the other axis has to do with time and it has to do with the time of our
35:50
ancestors and our descendants and I think about making art
35:56
as being part of a convers a a conversation that's been happening for
36:01
thousands of years and that will hopefully continue for many thousands
36:06
more and when you're looking at the sky or you're looking at the ocean you're
36:11
looking at a landscape that other artists have been looking at for thousands of years and so what you're
36:18
trying to do is you're trying to see what is different about that landscape
36:24
in your time and how is that different than what that landscape looked like to your ancestors and how might it look
36:32
different from your descendants how do the monsters that
36:38
live in our skies look different from those who presented themselves to our
36:46
ancestors in the
36:52
past so I've spent a lot of time trying to see the Landscapes that Define the present
36:58
and when I say seeing I don't just mean seeing with your eyes but also thinking
37:04
with materials thinking about how different materials are literally made
37:10
out of different histories and how those materials also have politics embedded
37:15
within them I did that as um as you mentioned for the don't follow the wind project
37:22
here in Japan with chimp pom and Jason W and canar ikigami and many others um I
37:28
made a sculpture and to make this sculpture we traveled to the exclusion Zone to collect IR radiated glass from
37:36
the disaster and then I traveled to New Mexico to the place where the first
37:41
nuclear weapon was detonated and when the first nuclear weapon was detonated
37:46
in the desert the explosion was hotter than the surface of the Sun and it
37:51
turned the surface of the desert into glass and I collected some of that glass
37:57
and I refused it together with the broken glass from the disaster here and we installed the sculpture back in the
38:04
exclusion zone so I spent a lot of time trying to
38:09
see Landscapes looking at these drones and cables and satellites and the machines that populate our planet but we
38:18
we should also remember that those machines are looking at us and that brings us to another one of my interests
38:29
for the past 20 years or so we've been completely surrounded by sensing systems
38:35
and imaging systems we're at a moment in history where most of the images in the world are made by machines for other
38:43
machines with humans very rarely in that
38:51
Loop and in my studio I've been developing tools that show us as humans
38:57
What machines are seeing when they look out at the world I've been trying to answer a
39:04
central question which is how do machines
39:13
see how do computers see the
39:19
world how do historical Landscapes look through the eyes of computer vision
39:26
systems and art artificial intelligence systems what kinds of mathematical
39:32
abstractions do computers turn Landscapes
39:38
into and what kinds of images do we use to teach computers how to
39:45
see the images that we use to teach computers how to see are called training
39:52
images and this is also something I've spent a lot of time looking at
39:58
sometimes training images are quite normal this is a slide from the most widely used of these data sets of
40:06
training images it's called imag net this is pretty normal they have a category called apple and inside the
40:13
data set they have many thousands of pictures of apples and you can use a data set like this to train a machine
40:20
learning model and use it to recognize things like oranges and apples and
40:25
bananas but when we start looking more closely at training data we often find that
40:32
there are some strange and disturbing things going
40:39
on it's a little strange this is even
40:45
stranger debtor a category for debtors how are we supposed to see what somebody's bank
40:52
account looks like by looking at a picture of their face
40:58
it gets worse and even worse training sets are
41:06
filled with all sorts of racist misogynistic and cruel images and
41:16
categories especially when it comes to pictures of people and so I wanted to show some of
41:23
these very disturbing ways that computers were being taught to understand people and I took inspiration
41:31
from a man uh from named Teo Canady and Mr Canady was the first
41:37
person to invent facial recognition systems and he did this in the late 196s
41:43
and early 1970s and although Mr Canady was a computer scientist he debuted his first
41:50
facial recognition system as an art project he did this at the Osaka World
41:56
Expo in 19 1970 and his project was called computer
42:01
physiy it was a system that would take your picture and it would tell you which celebrity you most look like either John
42:09
F Kennedy or Winston Churchill or Marilyn
42:15
Monroe I wanted to take this idea and update it into the 21st century to show
42:21
where this technology had gone and so I built an AI model based on the kinds of
42:26
people included in contemporary training sets it was a system that you could use to take your picture and it would tell
42:33
you what kind of person you were according to AI my
42:39
result a divorced man I have never been
42:47
married this turned into a kind of Internet phenomena and uh it sort of made the
42:55
point that I was trying to make
43:04
I was very frustrated with AI models and I wanted to imagine different kinds of models computer vision systems see in
43:12
ways that are very literal and I wanted to make models that that did not see the world in literal ways and so I started
43:19
building models based on literature poetry philosophy and my point was the
43:26
way that we actually see images is highly allegorical think about something like
43:32
Freud's interpretation of Dreams the meaning of an image like a window or
43:38
false teeth or a swollen throat for Freud has nothing to do with
43:44
architecture or oral hygiene so to do this I had to create my
43:51
own training libraries to create my own taxonomies and build my own neural
43:58
networks when I did this you could build very different kinds of computer vision
44:04
Systems computer vision systems that would see Fantastical
44:11
objects and I think about these models themselves as kinds of artworks digital
44:17
ways of seeing the world in radically different ways and so I started taking my models and running them backwards
44:24
instead of using them to see the world I started using them to generate new
44:30
kinds of images the vampire these are some of the
44:37
images that you see below uh downstairs a man the Tower of Babel a pair of
44:48
eyes I started building other models ones that don't classify images at all
44:54
but in turn but instead turn their abstractions into colors I wanted to build models that would see the world
45:00
without classifying it or without judging
45:07
it I want to leave you with a final thought the works that I have shown you
45:12
today and that are included in the exhibition ask questions about how do we see the technological infrastructures
45:18
around us how do those technological infrastructures see us but I want to
45:24
leave you with a question that I'm thinking of about now which is how do
45:30
Technologies and the interests behind them prompt us to see things that may or
45:38
may not be there and how do those things that may or may not be there nonetheless become
45:46
real in other words how do Technologies prompt us to hallucinate and how do our
45:52
hallucinations take on a life of Their Own thank
46:00
[Applause]
46:06
you thank you very much indeed Trevor
46:14
and we have all run side the over time so the talk will be about 30
46:22
minutes we will keep about 10 minutes for the final
46:28
q& day so we're going to keep to the
46:35
original ending time but we're going to be shuffling around with the actual time
46:40
breakdown the last time you met was five or six years ago before the pandemic am
46:46
I right oh
46:53
this so it's been five or six years since you met face to face compared to
46:59
the previous discussion um you've had five or six
47:04
years passed by there was a pandemic for three full
47:10
years two Wars have broken
47:15
out and there have been a number of
47:21
extremely important developments I was wondering how what were the past five
47:27
six years like for you could you go first [Music]
47:33
please difficult I mean I I we never really
47:41
exited that period you know I we're still living in it and there the changes
47:48
are quite radical and still ongoing so it's very hard to give any sort of final
47:54
pronouncements about them um yeah
48:00
I'm what have I been doing I learned programming and then Chad GPT came along
48:06
and did it on my behalf so I stopped so I stopped learning to program
48:13
now CH GPT does my programming what can I
48:21
say that's that's the five years in a nutshell
48:30
programming programming and chat GPT interesting um
48:38
Trevor you have stopped using AI in your
48:46
work I understand could you talk about some of the ways in which that happened
48:51
how did you come to stop using AI in your work yeah so when
48:57
AI was being developed a lot of the works that you see downstairs were from
49:03
a moment where AI hadn't really happened yet in
49:09
the way that it has now and what I was doing was reading research papers that
49:15
were coming out of Academia taking you know proposed algorithms proposed ways
49:21
of building models and building them myself in my studio and building models
49:28
from scratch more or less and so I really understood everything that I was
49:33
doing and and had enormous amount of control over what I wanted those models to be what I wanted them to do and could
49:41
do what could turn the technology into
49:47
something that I wanted it to be and I think that that moment has passed um
49:55
when you look at the scale of the models now they're just huge huge huge much
50:00
much bigger they're much more curated in way not actually on the front end but on
50:06
the back end so in a way I weirdly feel like it's much more limited what you can
50:12
do with them now than what it what you were able to do with them when you had to build them
50:18
yourself um so that on one hand the second part of that is
50:26
I just got bored like chat GPT is really boring to
50:32
me and um and it's not actually interesting and um you know with the
50:39
with a lot of the image generators that are out there I I I want to Echo I guess
50:45
what you were saying there is this kind of convergence on the a median kind of
50:51
aesthetic and what to me was exciting about the early days is that they were completely complely up and they
50:58
didn't work and you they were extremely glitchy and that you could you could have some control over the aesthetic
51:04
that you wanted but it would also produce things that were novel and surprising and you really did feel like
51:10
you were having a a dialogue and in in a way with these these very strange model
51:16
objects um so I so I've stopped using AI in my work just because I don't think it
51:23
is really the existing techn Oles generate things I can't get them to
51:30
generate things that I think are interesting they're very good at making memes but I don't think they're very good at making things that I find you
51:37
know that rise to the level of of being interesting artworks now having said
51:42
that that's not to say that I'm uninterested in AI I'm extremely interested in AI but I'm more thinking
51:49
about how it's changing our Collective psychology how it's changing our
51:55
relationship to images how it's changing our relationship to each other and thinking about the ways in which it is
52:03
increasingly being weaponized against us whether that's in the form of you know
52:10
the mass appropriation of the world's culture and the packaging of that
52:16
together by giant companies to sell back to us whether that is the creation of
52:21
this class of uh microw workers or whether that is the creation
52:28
of media environments that are increasingly more actively predatory in
52:34
terms of uh trying to extract value from us so yeah same I mean I also stopped
52:43
almost using any sort of so-called AI generated imagery I only used it once
52:50
recently when I had to generate something which looks really ugly so
52:55
that worked very well well um but otherwise you know there's even if you
53:02
go quite far into the technology you do have some controls ET Etc but you never
53:10
know what the initial Foundation model was being trained on right there could
53:16
be I don't know child pornography or something coming at you because it's all in there not only presumably but it has
53:24
also been shown that it's in there so in that sense you work with something which
53:31
you are not given the means to understand because most of it is in a black box it's
53:38
proprietary and uh the output is yeah it converges towards some kind
53:45
of uncontroversial optimized idea of Norm so that's not an
53:52
ideal artist tool I guess maybe in in the generation of sound there could be
53:59
some much more interesting um develop results but image
54:05
is uh at the moment not a very fruitful um area to work with with image
54:13
generators but as Trevor said I completely agree I think the political economy of AI is something which is vast
54:22
it is expanding it has extremely important an consequences for our lives
54:29
it's very relevant especially in relation to its energy consumption its
54:35
environmental output um the you know mining of
54:41
resources uh burning of electricity etc etc and the question is what for right
54:49
to generate more shrimp Jesus memes you know you have the feeling that you know
54:55
whole coal m are being fired up to produce more cats with hats and they
55:01
look horrible so yeah no but it's really the case so I really started also
55:07
thinking about this energy aspect and I refrained from generating and also the
55:13
interfaces are built in a way that you need to keep generating it's like a slot
55:19
machine right you do one image and it's not good so you try again and you try
55:24
again and you try and you try again because you have little controls to really adjust it the way you want and
55:31
then you end up burning a lot of resources and you never really get the result you wanted M yeah I wanted to ask
55:40
you I think last time we spoke mhm we were thinking about almost like a theory
55:47
of images you know this this idea of invisible images machines talking to
55:53
each other you we were thinking about the poor image you know a lot of I I
55:59
wanted to ask you the question of what is an image in
56:07
2024 is what or what is an image in relationship to technology in 20124 and
56:13
would you answer that question now differently than you would have answered that you know five
56:19
or six years ago you mentioned a few of the things in the um yeah I mean we're on in a sort of transition right so it's
56:25
not entirely different but things are moving in a different direction so for example I mean both you and me we're
56:32
trained I'm trained as a filmmaker you're trained as a photographer we are very much rooted in a sort of optical
56:39
Paradigm right there is a lens there's light coming through actual photons they
56:44
have a physical impact and so on but I think more and more we're moving into a
56:50
statistical um area but also in a thermodynamical paradigm right it's not
56:57
you mentioned that before what do you mean by the thermodynamic Paradigm yeah we are moving into an area where um
57:05
image making is no longer connected with light and Optics but with Statistics
57:12
power and energy so some of the models which are used inside the image
57:18
generators are literally built on something called the heat diffusion formula right so basic the
57:26
whole idea is to spend power and to heat things up right you also power do you
57:33
mean literally like power coming out of electricity electricity to or or any
57:38
other kind of power right and to heat things so that's kind of the general
57:44
idea behind this kind of um image generation you also see it in the kind
57:50
of vocabulary that CEOs use when talking about this uh technology they always um
57:58
compare it with fire you know AI is fire and we are kind of the people who bring
58:04
the fire from the gods to the humans etc etc and they they say that it's like
58:10
fire because they think it's the most important thing than since people
58:15
domesticated fire and started you know to heat and to cook Etc so in a way the
58:22
whole um field of thinking has shifted from light to heat and there we're also
58:29
in a very interesting neighborhood because you know these fields are very
58:36
close to for example Finance the heat diffusion formula is also something that's used in finance to
58:43
calculate future stock option prices Etc so basically the more you go into the
58:49
machine and into the mathematics you end up with the idea
58:54
that we completely left in a way the the field where vision is and Optics is the
59:03
main thing where somewhere else no and so that statistical convergence also explains this aesthetic convergence they
59:09
the same thing the the aesthetic convergence is just like the the visual representation of that statistical
59:16
convergence yes it's like you throw all data in a big pot and let it stew for 20
59:23
hours MH that's the I want to push on that a little bit because this is reminding me of the
59:29
globalization con con conversation in the 1990s which like oh McDonald's is
59:35
going to be everywhere and therefore the world will all look the same and you alluded to that a little bit with with
59:41
your pictures of cities but it was a little bit more complicated than that right yes because it's not and it's
59:48
interesting I used the image of this iPad of Apple you know to to illustrate
59:54
this idea because you have the feeling that everything is being squeezed into
59:59
one plane right so it's flat it looks flat but the consequence of that is that
1:00:05
there the whole Space of culture is being flattened into something which
1:00:11
only knows two directions up and down or left and right right because this plane
1:00:18
is not only flat but it also divides right what do you mean well there's only
1:00:24
if you if you think about This Plane then there's only two positions you can be left of that plane or right of the
1:00:31
plane so even though it's flat it's also polarizing I see what you're saying yeah
1:00:37
yeah so this this is so basically this flatness on the other hand creates these
1:00:43
massive polarization and conflicts because if people are only being exposed
1:00:48
to this kind of average imagery then they cannot tolerate any outliers right
1:00:57
I think there's a second Dynamic going on which is well which is the amplification of those outliers so you
1:01:05
you do have this aesthetic conver convergence but I think that that
1:01:13
convergence the inverse of that is the amplification of difference
1:01:19
here's what I mean by that I think that in a few years time you and I I may
1:01:26
watch a television show on Netflix and we're going to see a different version of that show because that show will be
1:01:32
you you have a metadata profile that's different than mine it will be custom generated be custom generated for you
1:01:39
right and what will be added into that show are things that you see or hear
1:01:47
that will be designed to sell you something or try to extract some kind of value from you or induce some kind of
1:01:55
activ in you so on on one hand you do have this convergence but at the same time you have the opport the uh a media
1:02:05
landscape that is primed to increasingly create your own world
1:02:12
for you yes right within certain parameters within the parameters of like
1:02:17
the economic parameters of trying to extract some kind of value from I don't think there is any sort of um
1:02:24
contradiction there even think that you know you can have different data worlds
1:02:29
which in fact already exist today right there are completely they are data silos
1:02:35
which are completely shut off from one another and so you cannot even access
1:02:41
this other data exactly yeah yeah so that averaging that that that
1:02:46
convergence would you say that it's not it's more of a political economy SL
1:02:54
athetic convergence to to me it seems it's it's not just an aesthetic
1:03:00
convergence it's an aesthetic convergence that is
1:03:06
motivated by an underlying economic relationship that you're going to have
1:03:12
to images yes the aesthetic is just the outcome right it's just the consequence
1:03:18
of these big data silos in which data do not longer circulate you know anywhere
1:03:25
El but just remain put for extraction and for creating all these variations of the
1:03:31
same because what you're describing these customade TV shows Etc are
1:03:37
basically variations of something that is essentially not changing yeah yeah I
1:03:45
always I was recently thinking about the quote I forgot who made it but it was a famous advertising executive who said
1:03:52
50% of advertising doesn't work the problem is we don't know which 50%
1:03:57
doesn't work right but with these newer kind of tools that that can get much more Amplified you can know that 99% of
1:04:05
it works for example yeah the consequence of that is that as you have a media environment that is much more
1:04:13
efficient economically you radically reduce the space
1:04:19
of weird stuff happening or that you radically reduce the space of the
1:04:25
possibility for radical culture I guess this is the
1:04:30
flattening I guess you know I also made notes when you spoke and you mentioned
1:04:36
changes to time right what kind of time space are we living in and the
1:04:43
interesting thing is that I think there there is this strange kind of tension
1:04:49
also in relation to the creation of time because each second there's years of
1:04:55
footage being uploaded to YouTube so the present is Multiplied a lot I calculated
1:05:02
that you know if one some one person wanted to watch one year on of uploads
1:05:08
to YouTube it takes 30 years and if you watch YouTube for 33 years then you will
1:05:15
already spend almost you know the entire living history so in a way time is
1:05:21
slowed down a lot inside these silos but because it's so inert in there it
1:05:28
creates a lot of tension elsewhere mhm do you do you you look like you want
1:05:34
to interrupt us maybe we we were afraid that we go off into a direction that
1:05:40
only interests us but no one else will be able to follow so please interrupt us
1:05:47
because we can go on for until tomorrow
1:05:53
more there's many questions here I money I think there is one question
1:05:58
about looking at images and what what does it mean to read an image now which
1:06:05
I think has radically changed actually um there's other things we can talk
1:06:12
about we can talk about this neurological aspect of images things as well
1:06:17
but if we just talk about the practice of looking at images now one of I've
1:06:23
noticed a lot of things one thing that I've noticed is that there's whole aesthetic histories that have been
1:06:30
Rewritten for me now in the sense like if I look at like synth pop images from
1:06:35
like the 80s or if I look at sci-fi like 1970s illustrations I read those now as
1:06:41
AI generated images right I just I look at I'm like oh that looks like an AI
1:06:46
thing now and what what a kind of a tragedy that is really right because is
1:06:52
this overy of of huge histories that I actually think are quite interesting so we have that on one
1:07:00
hand we have on the other hand this question of what's real versus not real
1:07:06
like when you're reading the internet now I very regularly reading some article like I don't know if this is an
1:07:12
AI thing or a human thing is that interesting to notice or
1:07:18
not that's another thing to think about as well is there a we were
1:07:28
we always kind of knew intellectually that we should not trust language or that we should not trust image but that
1:07:34
that that feels different now in the age of AI yeah I think the distrust is much
1:07:40
more Global now I mean I I realized I barely look at images anymore especially
1:07:47
news images I mean it's like why should I even spend time looking at them you
1:07:53
know they more or less lost the evidence value MH um so that's a huge huge shift
1:08:01
because I used to try to look at images very closely and at the details and so
1:08:08
on and right now I'm like why B why bother at all
1:08:15
mhm on this cultural flattening so you were
1:08:20
talking about a world in which AI models have kind of eaten all of culture and
1:08:27
ingested the history of images in order to create new ones that
1:08:33
are essentially regurgitations of things from the the training set pushing that
1:08:39
forward and that leading to a kind of
1:08:45
standstill in visual culture where it's sort of visual culture ends in 2024 what
1:08:52
have you and everything but you hear me me out M how is this feel
1:09:00
different than the warhall moment for lack of a better word you know I think
1:09:06
about the warhall moment I think I have I really have an answer because I
1:09:11
compare it to something maybe unexpected uh which is something from
1:09:17
Marx actually who called this the metabolic Rift what is the metabolic
1:09:23
Rift and I think we have something comparable now a digital Rift so what is
1:09:28
the metabolic Rift it's the moment when a sort of circulation within human economy stops
1:09:36
so what is this circulation um um yeah the produce from the fields
1:09:45
used to be taken to the city from the countryside then some people ate it then
1:09:51
they shed it out and the um exrs were taken back to the fields as fertilizers
1:09:59
so there was a circle and Mark said this circle was interrupted at that moment
1:10:05
when basically the waste was not taken back to the fields anymore and then it
1:10:11
started accumulating in the cities in form of you know untreated waste water
1:10:17
there was epidemics there was colera there was something called the Great stink in London right where the TS were
1:10:24
so polluted that people who fell in died immediately right so it created this
1:10:30
huge crisis which could only be solved after there was large infrastructural
1:10:36
Public Works to establish uh sewage etc etc so I think in a way we're in a
1:10:43
similar situation now with data where data do not really circulate anymore
1:10:50
right because they are being kept in proprietary silus MH they accumulate in
1:10:56
there and stagnate I I one could call it stinky data somebody called it hsur data
1:11:03
like it becomes increasingly inbred to the point ex H data yeah so you have a
1:11:10
sort of equivalent of a sanitation crisis right because these data accumulate they stagnate they don't
1:11:17
smell so well I guess and you would need at this point a huge effort you know
1:11:24
investing into infrastructure uh to create some sort of
1:11:29
circulation or metabolism again where data from one area could fertilize you
1:11:36
know ideas in another area but I do not see who is going to do this well and
1:11:42
you're seeing this in the a lot of the technology trade press where they're complaining about running out of data
1:11:47
right to build bigger models you need more data at this point if you've ingested the whole internet and you've
1:11:53
ingested every book that's ever been written where is that supposed to come from right on one hand and then the
1:12:01
problem of compute and you mentioned this metabolic Rift as as being having an ecological component to it as well
1:12:08
and and you know when you're looking at uh the amount of compute that's required to make the newer models it's incredible
1:12:15
it's just it literally is as you described before you know setting the world on fire in order to uh yeah for
1:12:22
what to make C with hats to make cats with
1:12:35
Hats it is now 3:40 and this is 10 minutes over time but uh we'd like to
1:12:41
thank everybody for joining us [Applause]
1:12:56
thank you for joining us and this is the end of today's discussion
